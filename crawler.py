"""
í¬ë¡¤ëŸ¬ ëª¨ë“ˆ
- íšŒì‚¬ í™ˆí˜ì´ì§€ì—ì„œ ê²€ì‚¬í•­ëª©/ê²€ì‚¬ì£¼ê¸° ì •ë³´ í¬ë¡¤ë§
- ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´ DBì— ì €ì¥
"""
import re
import requests
from bs4 import BeautifulSoup
import logging
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import traceback

from config import URL_MAPPING, INDUSTRY_MAPPING, ITEM_POPUP_MAPPING, NUTRITION_POPUP_MAPPING, GENERAL_POPUP_MAPPING, SECTION_FILTER, LOG_FILE, LOG_FORMAT
from models import (
    save_inspection_item,
    save_inspection_cycle,
    save_nutrition_info,
    save_crawl_log,
    init_database
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format=LOG_FORMAT,
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class Crawler:
    """ì›¹ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self._driver = None

    def _get_driver(self):
        """Selenium WebDriver ìƒì„± (í•„ìš”í•  ë•Œë§Œ)"""
        if self._driver is None:
            try:
                options = Options()
                options.add_argument("--headless=new")
                options.add_argument("--no-sandbox")
                options.add_argument("--disable-dev-shm-usage")
                options.add_argument("--disable-gpu")
                options.add_argument("--window-size=1920,1080")
                self._driver = webdriver.Chrome(options=options)
                logger.info("WebDriver ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.error(f"WebDriver ìƒì„± ì‹¤íŒ¨: {e}")
                raise
        return self._driver

    def close(self):
        """WebDriver ì¢…ë£Œ"""
        if self._driver:
            self._driver.quit()
            self._driver = None
            logger.info("WebDriver ì¢…ë£Œ")

    def crawl_inspection_items(self, category: str) -> int:
        """
        ê²€ì‚¬í•­ëª© í¬ë¡¤ë§ (Selenium ì‚¬ìš© - íŒì—… ìš”ì†Œì—ì„œ ë°ì´í„° ì¶”ì¶œ)

        Args:
            category: "ì‹í’ˆ" ë˜ëŠ” "ì¶•ì‚°"
        Returns:
            ì €ì¥ëœ í•­ëª© ìˆ˜
        """
        url = URL_MAPPING.get("ê²€ì‚¬í•­ëª©", {}).get(category)
        if not url:
            logger.error(f"ê²€ì‚¬í•­ëª© URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {category}")
            return 0

        target_id = ITEM_POPUP_MAPPING.get(category)
        if not target_id:
            logger.error(f"ê²€ì‚¬í•­ëª© íŒì—… IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {category}")
            return 0

        try:
            driver = self._get_driver()
            logger.info(f"ê²€ì‚¬í•­ëª© í¬ë¡¤ë§ ì‹œì‘: {category} (íŒì—… ID: {target_id})")

            driver.get(url)
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            soup = BeautifulSoup(driver.page_source, "html.parser")

            # íŒì—… ìš”ì†Œì—ì„œ í…Œì´ë¸” ì°¾ê¸°
            target_element = soup.find("div", class_="needpopup answerPop", id=target_id)
            if not target_element:
                logger.warning(f"ê²€ì‚¬í•­ëª© íŒì—… ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {category} (ID: {target_id})")
                return 0

            table = target_element.find("table")
            if not table:
                logger.warning(f"ê²€ì‚¬í•­ëª© í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {category}")
                return 0

            count = 0
            rows = table.find_all("tr")[1:]  # í—¤ë” ì œì™¸

            for row in rows:
                columns = row.find_all("td", recursive=False)
                if len(columns) >= 3:
                    food_type = columns[1].get_text(strip=True)
                    items = columns[2].get_text(strip=True)
                    if food_type and items:
                        save_inspection_item(category, food_type, items)
                        count += 1

            logger.info(f"ê²€ì‚¬í•­ëª© í¬ë¡¤ë§ ì™„ë£Œ: {category}, {count}ê°œ ì €ì¥")
            return count

        except Exception as e:
            logger.error(f"ê²€ì‚¬í•­ëª© í¬ë¡¤ë§ ì‹¤íŒ¨: {category}, {e}")
            logger.error(traceback.format_exc())
            return 0

    def crawl_inspection_cycles(self, category: str) -> int:
        """
        ê²€ì‚¬ì£¼ê¸° í¬ë¡¤ë§ (Selenium ì‚¬ìš© - ë™ì  í˜ì´ì§€)

        Args:
            category: "ì‹í’ˆ" ë˜ëŠ” "ì¶•ì‚°"
        Returns:
            ì €ì¥ëœ í•­ëª© ìˆ˜
        """
        url = URL_MAPPING.get("ê²€ì‚¬ì£¼ê¸°", {}).get(category)
        if not url:
            logger.error(f"ê²€ì‚¬ì£¼ê¸° URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {category}")
            return 0

        # ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” ì—…ì¢… í•„í„°ë§
        if category == "ì‹í’ˆ":
            industries = ["ì‹í’ˆì œì¡°ê°€ê³µì—…", "ì¦‰ì„íŒë§¤ì œì¡°ê°€ê³µì—…"]
        else:
            industries = ["ì¶•ì‚°ë¬¼ì œì¡°ê°€ê³µì—…", "ì¶•ì‚°ë¬¼ì¦‰ì„íŒë§¤ì œì¡°ê°€ê³µì—…"]

        total_count = 0

        try:
            driver = self._get_driver()
            logger.info(f"ê²€ì‚¬ì£¼ê¸° í¬ë¡¤ë§ ì‹œì‘: {category}")

            driver.get(url)
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            soup = BeautifulSoup(driver.page_source, "html.parser")

            for industry in industries:
                target_id = INDUSTRY_MAPPING.get(industry)
                if not target_id:
                    continue

                target_element = soup.find("div", class_="needpopup answerPop", id=target_id)
                if not target_element:
                    logger.warning(f"ê²€ì‚¬ì£¼ê¸° ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {industry}")
                    continue

                table = target_element.find("table")
                if not table:
                    continue

                rows = table.find_all("tr")[1:]  # í—¤ë” ì œì™¸

                for row in rows:
                    columns = row.find_all("td", recursive=False)
                    if len(columns) >= 4:
                        food_group = columns[1].get_text(strip=True)
                        food_type_text = columns[2].get_text(strip=True)
                        cycle = columns[3].get_text(strip=True)

                        # ì—¬ëŸ¬ ì‹í’ˆ ìœ í˜•ì´ ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ê²½ìš° ê°ê° ì €ì¥
                        food_types = [ft.strip() for ft in food_type_text.split(',')]
                        for food_type in food_types:
                            if food_type and cycle:
                                save_inspection_cycle(category, industry, food_group, food_type, cycle)
                                total_count += 1

            logger.info(f"ê²€ì‚¬ì£¼ê¸° í¬ë¡¤ë§ ì™„ë£Œ: {category}, {total_count}ê°œ ì €ì¥")
            return total_count

        except Exception as e:
            logger.error(f"ê²€ì‚¬ì£¼ê¸° í¬ë¡¤ë§ ì‹¤íŒ¨: {category}, {e}")
            logger.error(traceback.format_exc())
            return 0

    def crawl_nutrition_info(self) -> int:
        """
        ì˜ì–‘ì„±ë¶„ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§ (Selenium ì‚¬ìš©)

        Returns:
            ì €ì¥ëœ í•­ëª© ìˆ˜
        """
        total_count = 0

        try:
            driver = self._get_driver()
            logger.info("ì˜ì–‘ì„±ë¶„ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")

            for test_type, url in URL_MAPPING.get("ì˜ì–‘ì„±ë¶„ê²€ì‚¬", {}).items():
                target_id = NUTRITION_POPUP_MAPPING.get(test_type)
                if not target_id:
                    logger.warning(f"ì˜ì–‘ì„±ë¶„ê²€ì‚¬ íŒì—… IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {test_type}")
                    continue

                logger.info(f"ì˜ì–‘ì„±ë¶„ê²€ì‚¬ í¬ë¡¤ë§: {test_type} (URL: {url})")

                driver.get(url)
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )

                soup = BeautifulSoup(driver.page_source, "html.parser")

                # íŒì—… ìš”ì†Œì—ì„œ í…Œì´ë¸” ì°¾ê¸°
                target_element = soup.find("div", class_="needpopup answerPop", id=target_id)
                if not target_element:
                    logger.warning(f"ì˜ì–‘ì„±ë¶„ê²€ì‚¬ íŒì—… ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {test_type} (ID: {target_id})")
                    continue

                table = target_element.find("table")
                if not table:
                    logger.warning(f"ì˜ì–‘ì„±ë¶„ê²€ì‚¬ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {test_type}")
                    continue

                # í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (ë§í¬ í¬í•¨)
                rows = table.find_all("tr")
                table_data = []

                for row in rows:
                    cells = row.find_all(["th", "td"])
                    if cells:
                        row_data = self._extract_cell_data_with_links(cells)
                        table_data.append(row_data)

                if table_data:
                    # í…Œì´ë¸” ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    details = self._format_table_data(table_data)
                    save_nutrition_info("ì˜ì–‘ì„±ë¶„ê²€ì‚¬", test_type, details)
                    total_count += 1
                    logger.info(f"ì˜ì–‘ì„±ë¶„ê²€ì‚¬ ì €ì¥ ì™„ë£Œ: {test_type}")

            logger.info(f"ì˜ì–‘ì„±ë¶„ê²€ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ: {total_count}ê°œ ì €ì¥")
            return total_count

        except Exception as e:
            logger.error(f"ì˜ì–‘ì„±ë¶„ê²€ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return 0

    def _extract_cell_data_with_links(self, cells) -> list:
        """ì…€ì—ì„œ í…ìŠ¤íŠ¸ì™€ ë§í¬ë¥¼ í•¨ê»˜ ì¶”ì¶œ

        Returns:
            list of (text, url) tuples or just text strings
        """
        row_data = []
        base_url = "https://www.biofl.co.kr"

        for cell in cells:
            # ì…€ ë‚´ì˜ ëª¨ë“  ë§í¬ ì°¾ê¸°
            links = cell.find_all("a", href=True)

            # "ìì„¸íˆ ë³´ê¸°" ë§í¬ê°€ ìˆëŠ” ê²½ìš°
            detail_links = [link for link in links if "ìì„¸íˆ" in link.get_text()]

            if detail_links:
                # ë§í¬ í…ìŠ¤íŠ¸ë¥¼ ì œì™¸í•œ ì…€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                cell_text = cell.get_text(strip=True)
                # ì²« ë²ˆì§¸ "ìì„¸íˆ ë³´ê¸°" ë§í¬ì˜ URL
                href = detail_links[0].get("href", "")

                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if href and not href.startswith("http"):
                    if href.startswith("/"):
                        href = base_url + href
                    else:
                        href = base_url + "/" + href

                row_data.append((cell_text, href))
            else:
                # ë§í¬ê°€ ì—†ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸
                row_data.append(cell.get_text(strip=True))

        return row_data

    def _format_table_data(self, table_data: list, section_filter: str = None) -> str:
        """í…Œì´ë¸” ë°ì´í„°ë¥¼ í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            table_data: í…Œì´ë¸” ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            section_filter: íŠ¹ì • ì„¹ì…˜ë§Œ í¬í•¨í•  ê²½ìš° í•´ë‹¹ ì„¹ì…˜ í—¤ë” í…ìŠ¤íŠ¸
        """
        result = []
        in_target_section = section_filter is None  # í•„í„° ì—†ìœ¼ë©´ í•­ìƒ True
        current_section_header = None

        for row in table_data:
            # ì œëª© í–‰ ì œì™¸ (Që¡œ ì‹œì‘í•˜ëŠ” ì§ˆë¬¸ ë²ˆí˜¸)
            if row and re.match(r'^Q\d+\.', str(row[0])):
                continue

            if len(row) >= 2:
                header_text = str(row[0]) if not isinstance(row[0], tuple) else row[0][0]

                # ì„¹ì…˜ í•„í„°ê°€ ìˆëŠ” ê²½ìš°, ì„¹ì…˜ í—¤ë” ì²´í¬
                if section_filter:
                    if section_filter in header_text:
                        in_target_section = True
                        current_section_header = header_text
                    elif current_section_header and not header_text.startswith(" "):
                        # ìƒˆë¡œìš´ ì„¹ì…˜ ì‹œì‘ (íƒ€ê²Ÿ ì„¹ì…˜ì´ ì•„ë‹Œ ê²½ìš°)
                        if section_filter not in header_text:
                            in_target_section = False

                if not in_target_section:
                    continue

                # ì²« ë²ˆì§¸ ì—´ì´ í—¤ë”ì¸ ê²½ìš°
                cleaned_values = []
                for val in row[1:]:
                    # valì´ (text, url) íŠœí”Œì¸ ê²½ìš°
                    if isinstance(val, tuple):
                        text, url = val
                        cleaned = self._clean_text(text)
                        if cleaned and url:
                            cleaned_values.append(f"{cleaned}{{{{URL:{url}}}}}")
                        elif cleaned:
                            cleaned_values.append(cleaned)
                    else:
                        cleaned = self._clean_text(val)
                        if cleaned:
                            cleaned_values.append(cleaned)
                if cleaned_values:
                    result.append(f"[{header_text}] {' | '.join(cleaned_values)}")
            elif len(row) == 1:
                val = row[0]
                if isinstance(val, tuple):
                    text, url = val
                    cleaned = self._clean_text(text)
                    if cleaned and not re.match(r'^Q\d+\.', cleaned):
                        if url:
                            result.append(f"{cleaned}{{{{URL:{url}}}}}")
                        else:
                            result.append(cleaned)
                else:
                    cleaned = self._clean_text(val)
                    if cleaned and not re.match(r'^Q\d+\.', cleaned):
                        result.append(cleaned)
        return "\n".join(result)

    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°"""
        if not text:
            return ""

        # "ìì„¸íˆ ë³´ê¸°" ë° ê´€ë ¨ í…ìŠ¤íŠ¸ ì œê±°
        text = re.sub(r'ìì„¸íˆ\s*ë³´ê¸°', '', text)
        # ì•ë’¤ í•˜ì´í”ˆ, ê³µë°± ì •ë¦¬
        text = re.sub(r'^[-\s]+|[-\s]+$', '', text)
        # ì¤‘ë³µ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)

        return text.strip()

    def _extract_general_text(self, text: str) -> str:
        """ì¼ë°˜ í…ìŠ¤íŠ¸ í˜•ì‹ì˜ íŒì—… ë‚´ìš© ì¶”ì¶œ (ìê°€í’ˆì§ˆê²€ì‚¬ ë“±)"""
        if not text:
            return ""

        # Q ì œëª© ì œê±°
        text = re.sub(r'Q\d+\.\s*[^\n]*', '', text)

        # "ìì„¸íˆ ë³´ê¸°", "Close" ë“± ë²„íŠ¼ í…ìŠ¤íŠ¸ ì œê±°
        text = re.sub(r'ìì„¸íˆ\s*ë³´ê¸°', '', text)
        text = re.sub(r'\bClose\b', '', text)

        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()

        result = []

        # ì£¼ìš” ì„¹ì…˜ ë¶„ë¦¬
        # 1. â€» ì£¼ì˜ì‚¬í•­ ë¶„ë¦¬
        parts = re.split(r'(â€»[^â€»ê´€ë ¨]*)', text)

        for part in parts:
            part = part.strip()
            if not part:
                continue

            # â€» ì£¼ì˜ì‚¬í•­
            if part.startswith('â€»'):
                result.append(f"\n\nâš ï¸ ì£¼ì˜ì‚¬í•­")
                result.append(f"  {part}")
                continue

            # ê´€ë ¨ ë²•ë ¹ ìë£Œ ë¶„ë¦¬
            if 'ê´€ë ¨ ë²•ë ¹' in part:
                legal_split = re.split(r'(ê´€ë ¨ ë²•ë ¹[^:]*:[^\.]+\.)', part)
                for legal_part in legal_split:
                    legal_part = legal_part.strip()
                    if not legal_part:
                        continue
                    if legal_part.startswith('ê´€ë ¨ ë²•ë ¹'):
                        result.append(f"\n\nğŸ“‹ ê´€ë ¨ ë²•ë ¹")
                        # ë²•ë ¹ ë‚´ìš© ì •ë¦¬
                        legal_content = legal_part.replace('ê´€ë ¨ ë²•ë ¹ ìë£Œ :', '').strip()
                        result.append(f"  {legal_content}")
                    else:
                        # ì¼ë°˜ ë¬¸ì¥ ì²˜ë¦¬
                        self._format_general_sentences(legal_part, result)
            else:
                self._format_general_sentences(part, result)

        formatted = '\n'.join(result).strip()
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬ (3ê°œ ì´ìƒ -> 2ê°œ)
        formatted = re.sub(r'\n{3,}', '\n\n', formatted)
        return formatted

    def _format_general_sentences(self, text: str, result: list):
        """ì¼ë°˜ ë¬¸ì¥ë“¤ì„ í¬ë§·íŒ…í•˜ì—¬ result ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€"""
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = re.split(r'(?<=[ë‹¤ìš”ìŒë©ë‹ˆë‹¤]\.)\s*', text)

        for sent in sentences:
            sent = sent.strip()
            if not sent or len(sent) < 5:
                continue

            # - ë¡œ ì‹œì‘í•˜ëŠ” í•­ëª©
            if sent.startswith('-'):
                result.append(f"\nâ€¢ {sent[1:].strip()}")
            # ë²ˆí˜¸ë¡œ ì‹œì‘í•˜ëŠ” í•­ëª© (ì˜ˆ: 3. ìê°€í’ˆì§ˆê²€ì‚¬...)
            elif re.match(r'^\d+\.\s', sent):
                result.append(f"\n\nğŸ“Œ {sent}")
            # ë‹¤ë§Œ, ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì„œ ì¡°í•­
            elif sent.startswith('ë‹¤ë§Œ,'):
                result.append(f"\n  â”” {sent}")
            # ì˜ˆ) ë¡œ ì‹œì‘í•˜ëŠ” ì˜ˆì‹œ
            elif sent.startswith('ì˜ˆ)'):
                result.append(f"\nğŸ’¡ {sent}")
            else:
                # ì¼ë°˜ ë¬¸ì¥ - ì²« ë¬¸ì¥ì´ë©´ bullet point ì¶”ê°€
                if not result or result[-1].startswith('\n\n'):
                    result.append(f"\nâ€¢ {sent}")
                else:
                    result.append(f"\nâ€¢ {sent}")

    def _extract_section_text(self, text: str, section_filter: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ì„¹ì…˜ë§Œ ì¶”ì¶œ (ì†Œë¹„ê¸°í•œì„¤ì • ë“±)"""
        if not text or not section_filter:
            return ""

        logger.info(f"ì„¹ì…˜ í•„í„° ì ìš©: {section_filter}")

        # "Close" ë“± ë²„íŠ¼ í…ìŠ¤íŠ¸ ì œê±°
        text = re.sub(r'\bClose\b', '', text)

        # ê³µë°± ì •ë¦¬ (ì¤„ë°”ê¿ˆì€ ìœ ì§€)
        text = re.sub(r'[ \t]+', ' ', text).strip()

        # ì„¹ì…˜ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸° (ì˜ˆ: "1) ì‹¤ì¸¡ì‹¤í—˜" ë˜ëŠ” "2) ê°€ì†ì‹¤í—˜")
        start_match = re.search(rf'{re.escape(section_filter)}', text)
        if not start_match:
            logger.warning(f"ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {section_filter}")
            return ""

        # ì„¹ì…˜ ì‹œì‘ë¶€í„° ëê¹Œì§€ ì¶”ì¶œ
        section_start = start_match.start()

        # ë‹¤ìŒ ì„¹ì…˜ ë²ˆí˜¸ ì°¾ê¸° (ì˜ˆ: í˜„ì¬ 1)ì´ë©´ 2) ì°¾ê¸°, í˜„ì¬ 2)ì´ë©´ ëê¹Œì§€)
        current_num = re.match(r'(\d)\)', section_filter)
        if current_num:
            next_num = int(current_num.group(1)) + 1
            # ë‹¤ìŒ ì„¹ì…˜ íŒ¨í„´ (ê³µë°± ìœ ë¬´ ìƒê´€ì—†ì´ í•œê¸€ë¡œ ì‹œì‘í•˜ëŠ” ì„¹ì…˜)
            next_pattern = rf'{next_num}\)\s*[ê°€-í£]'
            next_match = re.search(next_pattern, text[section_start + 10:])
            if next_match:
                section_end = section_start + 10 + next_match.start()
            else:
                section_end = len(text)
        else:
            section_end = len(text)

        section_text = text[section_start:section_end].strip()
        logger.info(f"ì¶”ì¶œëœ ì„¹ì…˜ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(section_text)}")

        if not section_text:
            return ""

        lines = []

        # ì œëª© ì¶”ì¶œ (ì˜ˆ: "1) ì‹¤ì¸¡ì‹¤í—˜ (3ê°œì›”ì´ë‚´ ì œí’ˆ)")
        title_match = re.match(r'(\d\)\s*[ê°€-í£]+\s*\([^)]+\))', section_text)
        if title_match:
            title = title_match.group(1)
            # ì œëª© í¬ë§·íŒ…
            if "ì‹¤ì¸¡ì‹¤í—˜" in title:
                lines.append("ğŸ“‹ ì‹¤ì¸¡ì‹¤í—˜ (3ê°œì›” ì´ë‚´ ì œí’ˆ)")
            elif "ê°€ì†ì‹¤í—˜" in title:
                lines.append("ğŸ“‹ ê°€ì†ì‹¤í—˜ (3ê°œì›” ì´ìƒ ì œí’ˆ)")
            else:
                lines.append(f"ğŸ“‹ {title}")
            lines.append("")
            section_text = section_text[title_match.end():].strip()

        # ë³¸ë¬¸ ì„¤ëª…ê³¼ ì˜ˆì‹œ ë¶„ë¦¬
        example_split = re.split(r'ì˜ˆ\)\s*', section_text, maxsplit=1)

        # ë³¸ë¬¸ ì„¤ëª… ì¶”ì¶œ
        if example_split[0].strip():
            description = example_split[0].strip()
            # ê³µë°± ì •ë¦¬
            description = re.sub(r'\s+', ' ', description)

            # ì„¤ëª… í…ìŠ¤íŠ¸ í¬ë§·íŒ…
            lines.append("ğŸ“Œ ì„¤ëª…")
            lines.append(f"  {description}")
            lines.append("")

        # ì˜ˆì‹œ ì¶”ì¶œ
        if len(example_split) > 1 and example_split[1].strip():
            example = example_split[1].strip()
            # * ì´í›„ëŠ” ì°¸ê³ ì‚¬í•­ì´ë¯€ë¡œ ë¶„ë¦¬
            note_split = re.split(r'\s*\*\s*', example, maxsplit=1)
            example_text = note_split[0].strip()

            if example_text:
                # ê³µë°± ì •ë¦¬
                example_text = re.sub(r'\s+', ' ', example_text)
                lines.append("ğŸ’¡ ì˜ˆì‹œ")
                lines.append(f"  {example_text}")
                lines.append("")

            # ì°¸ê³ ì‚¬í•­
            if len(note_split) > 1 and note_split[1].strip():
                note = note_split[1].strip()
                note = re.sub(r'\s+', ' ', note)
                lines.append("âš ï¸ ì°¸ê³ ì‚¬í•­")
                lines.append(f"  * {note}")

        # ë³¸ë¬¸ì—ì„œ ì°¸ê³ ì‚¬í•­ ì¶”ì¶œ (ì˜ˆì‹œê°€ ì—†ëŠ” ê²½ìš°)
        if len(example_split) == 1:
            notes = re.findall(r'\*\s*([^*]+)', section_text)
            if notes:
                lines.append("âš ï¸ ì°¸ê³ ì‚¬í•­")
                for note in notes:
                    note = note.strip()
                    note = re.sub(r'\s+', ' ', note)
                    if note and len(note) > 3:
                        lines.append(f"  * {note}")

        result = '\n'.join(lines)
        # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
        result = re.sub(r'\n{3,}', '\n\n', result)
        logger.info(f"í¬ë§·íŒ…ëœ ê²°ê³¼ ê¸¸ì´: {len(result)}")
        return result

    def _extract_allergy_kit_section(self, text: str, section_filter: str) -> str:
        """ì•Œë ˆë¥´ê¸° í‚¤íŠ¸ ì„¹ì…˜ ì¶”ì¶œ (ELISA Kit ë˜ëŠ” RT-PCR Kit)"""
        if not text or not section_filter:
            return ""

        logger.info(f"ì•Œë ˆë¥´ê¸° ì„¹ì…˜ í•„í„° ì ìš©: {section_filter}")

        # "Close" ë“± ë²„íŠ¼ í…ìŠ¤íŠ¸ ì œê±°
        text = re.sub(r'\bClose\b', '', text)
        # Q ì œëª© ì œê±°
        text = re.sub(r'Q\d+\.[^Q]*?(?=ELISA|RT-PCR)', '', text, count=1)

        # ì„¹ì…˜ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
        start_match = re.search(rf'{re.escape(section_filter)}', text, re.IGNORECASE)
        if not start_match:
            logger.warning(f"ì•Œë ˆë¥´ê¸° ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {section_filter}")
            return ""

        section_start = start_match.start()

        # ë‹¤ìŒ ì„¹ì…˜ ì°¾ê¸° (ELISA Kit ë‹¤ìŒì— RT-PCR Kit, ë˜ëŠ” RT-PCR Kitì´ë©´ ëê¹Œì§€)
        if "ELISA" in section_filter.upper():
            # ELISA Kit ì„¹ì…˜ -> RT-PCR Kitê¹Œì§€
            next_match = re.search(r'RT-PCR\s*Kit', text[section_start + len(section_filter):], re.IGNORECASE)
            if next_match:
                section_end = section_start + len(section_filter) + next_match.start()
            else:
                section_end = len(text)
        else:
            # RT-PCR Kit ì„¹ì…˜ -> ëê¹Œì§€
            section_end = len(text)

        section_text = text[section_start:section_end].strip()
        logger.info(f"ì¶”ì¶œëœ ì•Œë ˆë¥´ê¸° ì„¹ì…˜ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(section_text)}")

        if not section_text:
            return ""

        lines = []

        # ì„¹ì…˜ ì œëª©
        if "ELISA" in section_filter.upper():
            lines.append("ğŸ§ª ELISA Kit")
        else:
            lines.append("ğŸ§ª RT-PCR Kit")
        lines.append("")

        # ë³´ìœ  Kit ì¶”ì¶œ
        kit_match = re.search(r'-\s*ë³´ìœ \s*[Kk]it\s*[:ï¼š]?\s*([^-]+?)(?=-\s*(?:ë³„ë„|ì…ê³ )|$)', section_text, re.DOTALL)
        if kit_match:
            kit_items = kit_match.group(1).strip()
            # ê³µë°± ì •ë¦¬
            kit_items = re.sub(r'\s+', ' ', kit_items)
            lines.append("ğŸ“Œ ë³´ìœ  Kit")
            lines.append(f"  {kit_items}")
            lines.append("")

        # ë³„ë„ ë¬¸ì˜ ì¶”ì¶œ
        inquiry_match = re.search(r'-\s*ë³„ë„\s*ë¬¸ì˜\s*[:ï¼š]?\s*([^-*]+?)(?=-|\*|$)', section_text, re.DOTALL)
        if inquiry_match:
            inquiry_items = inquiry_match.group(1).strip()
            inquiry_items = re.sub(r'\s+', ' ', inquiry_items)
            lines.append("ğŸ“Œ ë³„ë„ ë¬¸ì˜")
            lines.append(f"  {inquiry_items}")
            lines.append("")

        # ì…ê³  ì˜ˆì • ì¶”ì¶œ
        incoming_match = re.search(r'-\s*ì…ê³ \s*ì˜ˆì •\s*[:ï¼š]?\s*([^-\[*]+?)(?=-|\[|\*|$)', section_text, re.DOTALL)
        if incoming_match:
            incoming_items = incoming_match.group(1).strip()
            incoming_items = re.sub(r'\s+', ' ', incoming_items)
            lines.append("ğŸ“Œ ì…ê³  ì˜ˆì •")
            lines.append(f"  {incoming_items}")
            lines.append("")

        # ê²€ì¶œ ê°€ëŠ¥ ì¢… ì•ˆë‚´ (RT-PCRì—ë§Œ ìˆìŒ)
        detect_match = re.search(r'\[ê²€ì¶œ\s*ê°€ëŠ¥\s*ì¢…\s*ì•ˆë‚´\](.+?)(?=\*ê³ ê°ì§€ì›|$)', section_text, re.DOTALL)
        if detect_match:
            detect_content = detect_match.group(1).strip()
            lines.append("ğŸ“Œ ê²€ì¶œ ê°€ëŠ¥ ì¢… ì•ˆë‚´")

            # ê° ì¢…ë³„ë¡œ ë¶„ë¦¬ (1) ì˜¤ì§•ì–´, 2) ê²Œ, 3) ìƒˆìš°)
            species_pattern = r'(\d+\))\s*([ê°€-í£]+)\s*[:ï¼š]?\s*([^0-9]+?)(?=\d+\)|$)'
            species_matches = re.findall(species_pattern, detect_content)
            for num, name, detail in species_matches:
                detail = re.sub(r'\s+', ' ', detail).strip()
                if detail:
                    lines.append(f"  {num} {name}: {detail}")
            lines.append("")

        # * ì•ˆë‚´ì‚¬í•­ ì¶”ì¶œ
        note_match = re.search(r'\*\s*(?:ë³„ë„\s*ë¬¸ì˜í•˜ì‹ |ê³ ê°ì§€ì›)[^*]+', section_text)
        if note_match:
            note = note_match.group(0).strip()
            note = re.sub(r'\s+', ' ', note)
            lines.append("âš ï¸ ì•ˆë‚´ì‚¬í•­")
            lines.append(f"  {note}")

        result = '\n'.join(lines)
        logger.info(f"í¬ë§·íŒ…ëœ ì•Œë ˆë¥´ê¸° ê²°ê³¼ ê¸¸ì´: {len(result)}")
        return result

    def _extract_items_from_text(self, text: str, category: str = None, section_filter: str = None) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ í•­ëª©ë“¤ì„ ì¶”ì¶œí•˜ì—¬ í¬ë§·íŒ…"""
        if not text:
            return ""

        # ìê°€í’ˆì§ˆê²€ì‚¬ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ í˜•ì‹ ì‚¬ìš©
        if category == "ìê°€í’ˆì§ˆê²€ì‚¬":
            return self._extract_general_text(text)

        # ì†Œë¹„ê¸°í•œì„¤ì •ì€ ì„¹ì…˜ í•„í„° ì ìš©
        if category == "ì†Œë¹„ê¸°í•œì„¤ì •" and section_filter:
            return self._extract_section_text(text, section_filter)

        # ì•Œë ˆë¥´ê¸°ëŠ” í‚¤íŠ¸ ì„¹ì…˜ í•„í„° ì ìš©
        if category == "ì•Œë ˆë¥´ê¸°" and section_filter:
            return self._extract_allergy_kit_section(text, section_filter)

        # ì œëª© ì œê±° (Që¡œ ì‹œì‘í•˜ëŠ” ì§ˆë¬¸ ì œëª© ì „ì²´)
        # Q3.ë¹„ê±´(Vegan) ê²€ì‚¬ì˜ ì¢…ë¥˜ì™€ ì‹œë£ŒëŸ‰ ê°™ì€ ì œëª© ì „ì²´ ì œê±°
        text = re.sub(r'Q\d+\.[^Q]*?(?=DN|ê²€ì‚¬|Kit|í•„ìš”í•œ|í•´ë‹¹|ê°œë³„|\*|-|$)', '', text, count=1)

        # "ìì„¸íˆ ë³´ê¸°" ì œê±°
        text = re.sub(r'ìì„¸íˆ\s*ë³´ê¸°', '', text)

        result = []

        # DNìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í‚¤íŠ¸ëª… ì¶”ì¶œ (DNAnimal Screen, DNAnimal Ident ë“±)
        kit_pattern = r'(DN(?:Animal\s+(?:Screen|Ident)\s+[A-Za-z\s&]+(?:Kit)?)[^DN]*?)(?=DN|$|\*|í•„ìš”í•œ)'
        kits = re.findall(kit_pattern, text, re.IGNORECASE)

        if kits:
            result.append("ğŸ§ª ê²€ì‚¬ í‚¤íŠ¸")
            result.append("")
            for kit in kits:
                kit = kit.strip()
                kit = re.sub(r'\s+', ' ', kit)
                if kit and len(kit) > 5:
                    result.append(f"â€¢ {kit}")

        # í•„ìš”í•œ ì‹œë£ŒëŸ‰ ì¶”ì¶œ
        sample_match = re.search(r'(í•„ìš”í•œ\s*ì‹œë£ŒëŸ‰?[^*]*)', text)
        if sample_match:
            sample_info = sample_match.group(1).strip()
            sample_info = re.sub(r'\s+', ' ', sample_info)
            result.append(f"\nğŸ“¦ ì‹œë£ŒëŸ‰")
            result.append(f"â€¢ {sample_info}")

        # "-" ë¡œ ì‹œì‘í•˜ëŠ” í•­ëª© ì¶”ì¶œ (ì˜ˆ: - í•­ìƒë¬¼ì§ˆ 28ì¢…)
        dash_items = re.findall(r'-\s*([^-*\n]+?)(?=\s*-|\s*\*|$)', text)
        if dash_items:
            result.append("\nğŸ“‹ ê²€ì‚¬í•­ëª©")
            result.append("")
            for item in dash_items:
                item = item.strip()
                item = re.sub(r'\s+', ' ', item)
                if item and len(item) > 2:
                    result.append(f"â€¢ {item}")

        # * ë¡œ ì‹œì‘í•˜ëŠ” ì°¸ê³  ì‚¬í•­ ì¶”ì¶œ
        notes = re.findall(r'\*\s*([^*]+)', text)
        if notes:
            result.append("\nâš ï¸ ì°¸ê³ ì‚¬í•­")
            result.append("")
            for note in notes:
                note = note.strip()
                note = re.sub(r'\s+', ' ', note)
                if note and len(note) > 3:
                    result.append(f"â€¢ {note}")

        # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬í•´ì„œ ë°˜í™˜
        if not result:
            items = re.split(r'[-â€¢]\s*', text)
            for item in items:
                item = re.sub(r'\s+', ' ', item).strip()
                if item and len(item) > 3:
                    result.append(f"â€¢ {item}")

        return "\n".join(result) if result else text

    def crawl_general_info(self) -> int:
        """
        ì¼ë°˜ ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§ (ìê°€í’ˆì§ˆê²€ì‚¬, ì†Œë¹„ê¸°í•œì„¤ì •, í•­ìƒë¬¼ì§ˆ, ì”ë¥˜ë†ì•½, ë°©ì‚¬ëŠ¥, ë¹„ê±´, í• ë„, ë™ë¬¼DNA, ì•Œë ˆë¥´ê¸°, ê¸€ë£¨í…Free)

        Returns:
            ì €ì¥ëœ í•­ëª© ìˆ˜
        """
        total_count = 0
        categories = [
            "ìê°€í’ˆì§ˆê²€ì‚¬", "ì†Œë¹„ê¸°í•œì„¤ì •",
            "í•­ìƒë¬¼ì§ˆ", "ì”ë¥˜ë†ì•½", "ë°©ì‚¬ëŠ¥",
            "ë¹„ê±´", "í• ë„", "ë™ë¬¼DNA",
            "ì•Œë ˆë¥´ê¸°", "ê¸€ë£¨í…Free"
        ]

        try:
            driver = self._get_driver()
            logger.info("ì¼ë°˜ ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")

            for category in categories:
                category_urls = URL_MAPPING.get(category, {})
                category_popups = GENERAL_POPUP_MAPPING.get(category, {})
                category_filters = SECTION_FILTER.get(category, {})

                for menu_type, url in category_urls.items():
                    target_id = category_popups.get(menu_type)
                    if not target_id:
                        logger.warning(f"{category} íŒì—… IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {menu_type}")
                        continue

                    # ì„¹ì…˜ í•„í„° ê°€ì ¸ì˜¤ê¸°
                    section_filter = category_filters.get(menu_type)

                    logger.info(f"{category} í¬ë¡¤ë§: {menu_type} (URL: {url})" + (f" [í•„í„°: {section_filter}]" if section_filter else ""))

                    driver.get(url)
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )

                    soup = BeautifulSoup(driver.page_source, "html.parser")

                    # íŒì—… ìš”ì†Œì—ì„œ í…Œì´ë¸” ì°¾ê¸°
                    target_element = soup.find("div", class_="needpopup answerPop", id=target_id)
                    if not target_element:
                        logger.warning(f"{category} íŒì—… ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {menu_type} (ID: {target_id})")
                        continue

                    table = target_element.find("table")
                    if not table:
                        # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ ì •ì œ
                        raw_text = target_element.get_text(strip=True)
                        if raw_text:
                            details = self._extract_items_from_text(raw_text, category, section_filter)
                            if details:
                                save_nutrition_info(category, menu_type, details)
                                total_count += 1
                                logger.info(f"{category} ì €ì¥ ì™„ë£Œ: {menu_type} (í…ìŠ¤íŠ¸)")
                        continue

                    # í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (ë§í¬ í¬í•¨)
                    rows = table.find_all("tr")
                    table_data = []

                    for row in rows:
                        cells = row.find_all(["th", "td"])
                        if cells:
                            row_data = self._extract_cell_data_with_links(cells)
                            table_data.append(row_data)

                    if table_data:
                        # ì„¹ì…˜ í•„í„° ì ìš©í•˜ì—¬ í¬ë§·íŒ…
                        details = self._format_table_data(table_data, section_filter)
                        if details:  # í•„í„° ì ìš© í›„ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
                            save_nutrition_info(category, menu_type, details)
                            total_count += 1
                            logger.info(f"{category} ì €ì¥ ì™„ë£Œ: {menu_type}")

            logger.info(f"ì¼ë°˜ ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§ ì™„ë£Œ: {total_count}ê°œ ì €ì¥")
            return total_count

        except Exception as e:
            logger.error(f"ì¼ë°˜ ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return 0

    def crawl_all(self):
        """ëª¨ë“  ë°ì´í„° í¬ë¡¤ë§"""
        logger.info("=" * 50)
        logger.info("ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
        logger.info("=" * 50)

        total = 0

        # ê²€ì‚¬í•­ëª© í¬ë¡¤ë§ (requests - ê°€ë²¼ì›€)
        for category in ["ì‹í’ˆ", "ì¶•ì‚°"]:
            count = self.crawl_inspection_items(category)
            total += count

        # ê²€ì‚¬ì£¼ê¸° í¬ë¡¤ë§ (Selenium - ë™ì  í˜ì´ì§€)
        for category in ["ì‹í’ˆ", "ì¶•ì‚°"]:
            count = self.crawl_inspection_cycles(category)
            total += count

        # ì˜ì–‘ì„±ë¶„ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§
        count = self.crawl_nutrition_info()
        total += count

        # ì¼ë°˜ ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§ (í•­ìƒë¬¼ì§ˆ, ì”ë¥˜ë†ì•½, ë°©ì‚¬ëŠ¥, ë¹„ê±´, í• ë„, ë™ë¬¼DNA)
        count = self.crawl_general_info()
        total += count

        # WebDriver ì¢…ë£Œ
        self.close()

        # í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥
        if total > 0:
            save_crawl_log("all", "success", f"ì´ {total}ê°œ ë°ì´í„° ì €ì¥")
            logger.info(f"ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {total}ê°œ ë°ì´í„° ì €ì¥")
        else:
            save_crawl_log("all", "failed", "ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
            logger.error("í¬ë¡¤ë§ ì‹¤íŒ¨: ì €ì¥ëœ ë°ì´í„° ì—†ìŒ")

        return total


def run_crawler():
    """í¬ë¡¤ëŸ¬ ì‹¤í–‰ (ì™¸ë¶€ì—ì„œ í˜¸ì¶œìš©)"""
    init_database()
    crawler = Crawler()
    try:
        return crawler.crawl_all()
    finally:
        crawler.close()


if __name__ == "__main__":
    run_crawler()

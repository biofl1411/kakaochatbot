"""
í¬ë¡¤ëŸ¬ ëª¨ë“ˆ
- íšŒì‚¬ í™ˆí˜ì´ì§€ì—ì„œ ê²€ì‚¬í•­ëª©/ê²€ì‚¬ì£¼ê¸° ì •ë³´ í¬ë¡¤ë§
- ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´ DBì— ì €ì¥
"""
import re
import requests
from bs4 import BeautifulSoup
import logging
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import traceback

from config import URL_MAPPING, INDUSTRY_MAPPING, ITEM_POPUP_MAPPING, NUTRITION_POPUP_MAPPING, GENERAL_POPUP_MAPPING, SECTION_FILTER, LOG_FILE, LOG_FORMAT
from models import (
    save_inspection_item,
    save_inspection_cycle,
    save_nutrition_info,
    save_crawl_log,
    init_database
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format=LOG_FORMAT,
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class Crawler:
    """ì›¹ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self._driver = None

    def _get_driver(self):
        """Selenium WebDriver ìƒì„± (í•„ìš”í•  ë•Œë§Œ)"""
        if self._driver is None:
            try:
                options = Options()
                options.add_argument("--headless=new")
                options.add_argument("--no-sandbox")
                options.add_argument("--disable-dev-shm-usage")
                options.add_argument("--disable-gpu")
                options.add_argument("--window-size=1920,1080")
                self._driver = webdriver.Chrome(options=options)
                logger.info("WebDriver ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.error(f"WebDriver ìƒì„± ì‹¤íŒ¨: {e}")
                raise
        return self._driver

    def close(self):
        """WebDriver ì¢…ë£Œ"""
        if self._driver:
            self._driver.quit()
            self._driver = None
            logger.info("WebDriver ì¢…ë£Œ")

    def crawl_inspection_items(self, category: str) -> int:
        """
        ê²€ì‚¬í•­ëª© í¬ë¡¤ë§ (Selenium ì‚¬ìš© - íŒì—… ìš”ì†Œì—ì„œ ë°ì´í„° ì¶”ì¶œ)

        Args:
            category: "ì‹í’ˆ" ë˜ëŠ” "ì¶•ì‚°"
        Returns:
            ì €ì¥ëœ í•­ëª© ìˆ˜
        """
        url = URL_MAPPING.get("ê²€ì‚¬í•­ëª©", {}).get(category)
        if not url:
            logger.error(f"ê²€ì‚¬í•­ëª© URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {category}")
            return 0

        target_id = ITEM_POPUP_MAPPING.get(category)
        if not target_id:
            logger.error(f"ê²€ì‚¬í•­ëª© íŒì—… IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {category}")
            return 0

        try:
            driver = self._get_driver()
            logger.info(f"ê²€ì‚¬í•­ëª© í¬ë¡¤ë§ ì‹œì‘: {category} (íŒì—… ID: {target_id})")

            driver.get(url)
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            soup = BeautifulSoup(driver.page_source, "html.parser")

            # íŒì—… ìš”ì†Œì—ì„œ í…Œì´ë¸” ì°¾ê¸°
            target_element = soup.find("div", class_="needpopup answerPop", id=target_id)
            if not target_element:
                logger.warning(f"ê²€ì‚¬í•­ëª© íŒì—… ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {category} (ID: {target_id})")
                return 0

            table = target_element.find("table")
            if not table:
                logger.warning(f"ê²€ì‚¬í•­ëª© í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {category}")
                return 0

            count = 0
            rows = table.find_all("tr")[1:]  # í—¤ë” ì œì™¸

            for row in rows:
                columns = row.find_all("td", recursive=False)
                if len(columns) >= 3:
                    food_type = columns[1].get_text(strip=True)
                    items = columns[2].get_text(strip=True)
                    if food_type and items:
                        save_inspection_item(category, food_type, items)
                        count += 1

            logger.info(f"ê²€ì‚¬í•­ëª© í¬ë¡¤ë§ ì™„ë£Œ: {category}, {count}ê°œ ì €ì¥")
            return count

        except Exception as e:
            logger.error(f"ê²€ì‚¬í•­ëª© í¬ë¡¤ë§ ì‹¤íŒ¨: {category}, {e}")
            logger.error(traceback.format_exc())
            return 0

    def crawl_inspection_cycles(self, category: str) -> int:
        """
        ê²€ì‚¬ì£¼ê¸° í¬ë¡¤ë§ (Selenium ì‚¬ìš© - ë™ì  í˜ì´ì§€)

        Args:
            category: "ì‹í’ˆ" ë˜ëŠ” "ì¶•ì‚°"
        Returns:
            ì €ì¥ëœ í•­ëª© ìˆ˜
        """
        url = URL_MAPPING.get("ê²€ì‚¬ì£¼ê¸°", {}).get(category)
        if not url:
            logger.error(f"ê²€ì‚¬ì£¼ê¸° URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {category}")
            return 0

        # ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” ì—…ì¢… í•„í„°ë§
        if category == "ì‹í’ˆ":
            industries = ["ì‹í’ˆì œì¡°ê°€ê³µì—…", "ì¦‰ì„íŒë§¤ì œì¡°ê°€ê³µì—…"]
        else:
            industries = ["ì¶•ì‚°ë¬¼ì œì¡°ê°€ê³µì—…", "ì¶•ì‚°ë¬¼ì¦‰ì„íŒë§¤ì œì¡°ê°€ê³µì—…"]

        total_count = 0

        try:
            driver = self._get_driver()
            logger.info(f"ê²€ì‚¬ì£¼ê¸° í¬ë¡¤ë§ ì‹œì‘: {category}")

            driver.get(url)
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            soup = BeautifulSoup(driver.page_source, "html.parser")

            for industry in industries:
                target_id = INDUSTRY_MAPPING.get(industry)
                if not target_id:
                    continue

                target_element = soup.find("div", class_="needpopup answerPop", id=target_id)
                if not target_element:
                    logger.warning(f"ê²€ì‚¬ì£¼ê¸° ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {industry}")
                    continue

                table = target_element.find("table")
                if not table:
                    continue

                rows = table.find_all("tr")[1:]  # í—¤ë” ì œì™¸

                for row in rows:
                    columns = row.find_all("td", recursive=False)
                    if len(columns) >= 4:
                        food_group = columns[1].get_text(strip=True)
                        food_type_text = columns[2].get_text(strip=True)
                        cycle = columns[3].get_text(strip=True)

                        # ì—¬ëŸ¬ ì‹í’ˆ ìœ í˜•ì´ ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ê²½ìš° ê°ê° ì €ì¥
                        food_types = [ft.strip() for ft in food_type_text.split(',')]
                        for food_type in food_types:
                            if food_type and cycle:
                                save_inspection_cycle(category, industry, food_group, food_type, cycle)
                                total_count += 1

            logger.info(f"ê²€ì‚¬ì£¼ê¸° í¬ë¡¤ë§ ì™„ë£Œ: {category}, {total_count}ê°œ ì €ì¥")
            return total_count

        except Exception as e:
            logger.error(f"ê²€ì‚¬ì£¼ê¸° í¬ë¡¤ë§ ì‹¤íŒ¨: {category}, {e}")
            logger.error(traceback.format_exc())
            return 0

    def crawl_nutrition_info(self) -> int:
        """
        ì˜ì–‘ì„±ë¶„ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§ (Selenium ì‚¬ìš©)

        Returns:
            ì €ì¥ëœ í•­ëª© ìˆ˜
        """
        total_count = 0

        try:
            driver = self._get_driver()
            logger.info("ì˜ì–‘ì„±ë¶„ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")

            for test_type, url in URL_MAPPING.get("ì˜ì–‘ì„±ë¶„ê²€ì‚¬", {}).items():
                target_id = NUTRITION_POPUP_MAPPING.get(test_type)
                if not target_id:
                    logger.warning(f"ì˜ì–‘ì„±ë¶„ê²€ì‚¬ íŒì—… IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {test_type}")
                    continue

                logger.info(f"ì˜ì–‘ì„±ë¶„ê²€ì‚¬ í¬ë¡¤ë§: {test_type} (URL: {url})")

                driver.get(url)
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )

                soup = BeautifulSoup(driver.page_source, "html.parser")

                # íŒì—… ìš”ì†Œì—ì„œ í…Œì´ë¸” ì°¾ê¸°
                target_element = soup.find("div", class_="needpopup answerPop", id=target_id)
                if not target_element:
                    logger.warning(f"ì˜ì–‘ì„±ë¶„ê²€ì‚¬ íŒì—… ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {test_type} (ID: {target_id})")
                    continue

                table = target_element.find("table")
                if not table:
                    logger.warning(f"ì˜ì–‘ì„±ë¶„ê²€ì‚¬ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {test_type}")
                    continue

                # í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (ë§í¬ í¬í•¨)
                rows = table.find_all("tr")
                table_data = []

                for row in rows:
                    cells = row.find_all(["th", "td"])
                    if cells:
                        row_data = self._extract_cell_data_with_links(cells)
                        table_data.append(row_data)

                if table_data:
                    # í…Œì´ë¸” ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    details = self._format_table_data(table_data)
                    save_nutrition_info("ì˜ì–‘ì„±ë¶„ê²€ì‚¬", test_type, details)
                    total_count += 1
                    logger.info(f"ì˜ì–‘ì„±ë¶„ê²€ì‚¬ ì €ì¥ ì™„ë£Œ: {test_type}")

            logger.info(f"ì˜ì–‘ì„±ë¶„ê²€ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ: {total_count}ê°œ ì €ì¥")
            return total_count

        except Exception as e:
            logger.error(f"ì˜ì–‘ì„±ë¶„ê²€ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return 0

    def _extract_cell_data_with_links(self, cells) -> list:
        """ì…€ì—ì„œ í…ìŠ¤íŠ¸ì™€ ë§í¬ë¥¼ í•¨ê»˜ ì¶”ì¶œ

        Returns:
            list of (text, url) tuples or just text strings
        """
        row_data = []
        base_url = "https://www.biofl.co.kr"

        for cell in cells:
            # ì…€ ë‚´ì˜ ëª¨ë“  ë§í¬ ì°¾ê¸°
            links = cell.find_all("a", href=True)

            # "ìì„¸íˆ ë³´ê¸°" ë§í¬ê°€ ìˆëŠ” ê²½ìš°
            detail_links = [link for link in links if "ìì„¸íˆ" in link.get_text()]

            if detail_links:
                # ë§í¬ í…ìŠ¤íŠ¸ë¥¼ ì œì™¸í•œ ì…€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                cell_text = cell.get_text(strip=True)
                # ì²« ë²ˆì§¸ "ìì„¸íˆ ë³´ê¸°" ë§í¬ì˜ URL
                href = detail_links[0].get("href", "")

                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if href and not href.startswith("http"):
                    if href.startswith("/"):
                        href = base_url + href
                    else:
                        href = base_url + "/" + href

                row_data.append((cell_text, href))
            else:
                # ë§í¬ê°€ ì—†ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸
                row_data.append(cell.get_text(strip=True))

        return row_data

    def _format_table_data(self, table_data: list, section_filter: str = None) -> str:
        """í…Œì´ë¸” ë°ì´í„°ë¥¼ í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            table_data: í…Œì´ë¸” ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            section_filter: íŠ¹ì • ì„¹ì…˜ë§Œ í¬í•¨í•  ê²½ìš° í•´ë‹¹ ì„¹ì…˜ í—¤ë” í…ìŠ¤íŠ¸
        """
        result = []
        in_target_section = section_filter is None  # í•„í„° ì—†ìœ¼ë©´ í•­ìƒ True
        current_section_header = None

        for row in table_data:
            # ì œëª© í–‰ ì œì™¸ (Që¡œ ì‹œì‘í•˜ëŠ” ì§ˆë¬¸ ë²ˆí˜¸)
            if row and re.match(r'^Q\d+\.', str(row[0])):
                continue

            if len(row) >= 2:
                header_text = str(row[0]) if not isinstance(row[0], tuple) else row[0][0]

                # ì„¹ì…˜ í•„í„°ê°€ ìˆëŠ” ê²½ìš°, ì„¹ì…˜ í—¤ë” ì²´í¬
                if section_filter:
                    if section_filter in header_text:
                        in_target_section = True
                        current_section_header = header_text
                    elif current_section_header and not header_text.startswith(" "):
                        # ìƒˆë¡œìš´ ì„¹ì…˜ ì‹œì‘ (íƒ€ê²Ÿ ì„¹ì…˜ì´ ì•„ë‹Œ ê²½ìš°)
                        if section_filter not in header_text:
                            in_target_section = False

                if not in_target_section:
                    continue

                # ì²« ë²ˆì§¸ ì—´ì´ í—¤ë”ì¸ ê²½ìš°
                cleaned_values = []
                for val in row[1:]:
                    # valì´ (text, url) íŠœí”Œì¸ ê²½ìš°
                    if isinstance(val, tuple):
                        text, url = val
                        cleaned = self._clean_text(text)
                        if cleaned and url:
                            cleaned_values.append(f"{cleaned}{{{{URL:{url}}}}}")
                        elif cleaned:
                            cleaned_values.append(cleaned)
                    else:
                        cleaned = self._clean_text(val)
                        if cleaned:
                            cleaned_values.append(cleaned)
                if cleaned_values:
                    result.append(f"[{header_text}] {' | '.join(cleaned_values)}")
            elif len(row) == 1:
                val = row[0]
                if isinstance(val, tuple):
                    text, url = val
                    cleaned = self._clean_text(text)
                    if cleaned and not re.match(r'^Q\d+\.', cleaned):
                        if url:
                            result.append(f"{cleaned}{{{{URL:{url}}}}}")
                        else:
                            result.append(cleaned)
                else:
                    cleaned = self._clean_text(val)
                    if cleaned and not re.match(r'^Q\d+\.', cleaned):
                        result.append(cleaned)
        return "\n".join(result)

    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°"""
        if not text:
            return ""

        # "ìì„¸íˆ ë³´ê¸°" ë° ê´€ë ¨ í…ìŠ¤íŠ¸ ì œê±°
        text = re.sub(r'ìì„¸íˆ\s*ë³´ê¸°', '', text)
        # ì•ë’¤ í•˜ì´í”ˆ, ê³µë°± ì •ë¦¬
        text = re.sub(r'^[-\s]+|[-\s]+$', '', text)
        # ì¤‘ë³µ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)

        return text.strip()

    def _extract_general_text(self, text: str) -> str:
        """ì¼ë°˜ í…ìŠ¤íŠ¸ í˜•ì‹ì˜ íŒì—… ë‚´ìš© ì¶”ì¶œ (ìê°€í’ˆì§ˆê²€ì‚¬ ë“±)"""
        if not text:
            return ""

        # Q ì œëª© ì œê±°
        text = re.sub(r'Q\d+\.\s*[^\n]*', '', text)

        # "ìì„¸íˆ ë³´ê¸°", "Close" ë“± ë²„íŠ¼ í…ìŠ¤íŠ¸ ì œê±°
        text = re.sub(r'ìì„¸íˆ\s*ë³´ê¸°', '', text)
        text = re.sub(r'\bClose\b', '', text)

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ, ë‹¤. ë“±ìœ¼ë¡œ ëë‚˜ëŠ” ë¬¸ì¥)
        sentences = re.split(r'(?<=[ë‹¤ìš”ìŒë©ë‹ˆê¹Œ]\.)\s*', text)

        result = []
        for sent in sentences:
            sent = sent.strip()
            sent = re.sub(r'\s+', ' ', sent)
            if sent and len(sent) > 5:
                # - ë¡œ ì‹œì‘í•˜ëŠ” í•­ëª©
                if sent.startswith('-'):
                    result.append(f"\nâ€¢ {sent[1:].strip()}")
                # â€» ë¡œ ì‹œì‘í•˜ëŠ” ì£¼ì˜ì‚¬í•­
                elif sent.startswith('â€»'):
                    result.append(f"\nâš ï¸ {sent}")
                # ê´€ë ¨ ë²•ë ¹
                elif sent.startswith('ê´€ë ¨ ë²•ë ¹'):
                    result.append(f"\n\nğŸ“‹ {sent}")
                # ì˜ˆ) ë¡œ ì‹œì‘í•˜ëŠ” ì˜ˆì‹œ
                elif sent.startswith('ì˜ˆ)'):
                    result.append(f"\nğŸ’¡ {sent}")
                else:
                    result.append(f"\n{sent}")

        formatted = '\n'.join(result).strip()
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬ (3ê°œ ì´ìƒ -> 2ê°œ)
        formatted = re.sub(r'\n{3,}', '\n\n', formatted)
        return formatted

    def _extract_section_text(self, text: str, section_filter: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ì„¹ì…˜ë§Œ ì¶”ì¶œ (ì†Œë¹„ê¸°í•œì„¤ì • ë“±)"""
        if not text or not section_filter:
            return ""

        # Q ì œëª© ì œê±°
        text = re.sub(r'Q\d+\.\s*[^1-9]*?(?=\d\))', '', text)

        # "Close" ë“± ë²„íŠ¼ í…ìŠ¤íŠ¸ ì œê±°
        text = re.sub(r'\bClose\b', '', text)

        # ì„¹ì…˜ íŒ¨í„´ ì°¾ê¸° (ì˜ˆ: "1) ì‹¤ì¸¡ì‹¤í—˜" ë˜ëŠ” "2) ê°€ì†ì‹¤í—˜")
        section_pattern = rf'({re.escape(section_filter)}[^)]*\)?\s*[^\d]*?)(?=\d\)\s|\Z)'
        match = re.search(section_pattern, text, re.DOTALL)

        if match:
            section_text = match.group(1).strip()
            # ì¤„ë°”ê¿ˆ ì •ë¦¬
            section_text = re.sub(r'\s+', ' ', section_text)
            lines = []

            # ì œëª© ì¶”ì¶œ (ì˜ˆ: "1) ì‹¤ì¸¡ì‹¤í—˜ (3ê°œì›”ì´ë‚´ ì œí’ˆ)")
            title_match = re.match(r'(\d\)\s*[^\(]+\([^)]+\))', section_text)
            if title_match:
                lines.append(f"ğŸ“‹ {title_match.group(1)}")
                lines.append("")  # ì œëª© í›„ ë¹ˆ ì¤„
                section_text = section_text[title_match.end():]

            # ë‚˜ë¨¸ì§€ ë‚´ìš©ì„ ë¬¸ì¥ë³„ë¡œ ì •ë¦¬
            sentences = re.split(r'(?<=[ë‹¤ìš”]\.)\s*', section_text)
            for sent in sentences:
                sent = sent.strip()
                if sent and len(sent) > 3:
                    if sent.startswith('ì˜ˆ)'):
                        lines.append(f"\nğŸ’¡ ì˜ˆì‹œ")
                        lines.append(f"  {sent[2:].strip()}")
                    else:
                        lines.append(f"â€¢ {sent}")

            return '\n'.join(lines)

        return ""

    def _extract_items_from_text(self, text: str, category: str = None, section_filter: str = None) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ í•­ëª©ë“¤ì„ ì¶”ì¶œí•˜ì—¬ í¬ë§·íŒ…"""
        if not text:
            return ""

        # ìê°€í’ˆì§ˆê²€ì‚¬ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ í˜•ì‹ ì‚¬ìš©
        if category == "ìê°€í’ˆì§ˆê²€ì‚¬":
            return self._extract_general_text(text)

        # ì†Œë¹„ê¸°í•œì„¤ì •ì€ ì„¹ì…˜ í•„í„° ì ìš©
        if category == "ì†Œë¹„ê¸°í•œì„¤ì •" and section_filter:
            return self._extract_section_text(text, section_filter)

        # ì œëª© ì œê±° (Që¡œ ì‹œì‘í•˜ëŠ” ì§ˆë¬¸ ì œëª© ì „ì²´)
        # Q3.ë¹„ê±´(Vegan) ê²€ì‚¬ì˜ ì¢…ë¥˜ì™€ ì‹œë£ŒëŸ‰ ê°™ì€ ì œëª© ì „ì²´ ì œê±°
        text = re.sub(r'Q\d+\.[^Q]*?(?=DN|ê²€ì‚¬|Kit|í•„ìš”í•œ|í•´ë‹¹|ê°œë³„|\*|-|$)', '', text, count=1)

        # "ìì„¸íˆ ë³´ê¸°" ì œê±°
        text = re.sub(r'ìì„¸íˆ\s*ë³´ê¸°', '', text)

        result = []

        # DNìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í‚¤íŠ¸ëª… ì¶”ì¶œ (DNAnimal Screen, DNAnimal Ident ë“±)
        kit_pattern = r'(DN(?:Animal\s+(?:Screen|Ident)\s+[A-Za-z\s&]+(?:Kit)?)[^DN]*?)(?=DN|$|\*|í•„ìš”í•œ)'
        kits = re.findall(kit_pattern, text, re.IGNORECASE)

        if kits:
            result.append("ğŸ§ª ê²€ì‚¬ í‚¤íŠ¸")
            result.append("")
            for kit in kits:
                kit = kit.strip()
                kit = re.sub(r'\s+', ' ', kit)
                if kit and len(kit) > 5:
                    result.append(f"â€¢ {kit}")

        # í•„ìš”í•œ ì‹œë£ŒëŸ‰ ì¶”ì¶œ
        sample_match = re.search(r'(í•„ìš”í•œ\s*ì‹œë£ŒëŸ‰?[^*]*)', text)
        if sample_match:
            sample_info = sample_match.group(1).strip()
            sample_info = re.sub(r'\s+', ' ', sample_info)
            result.append(f"\nğŸ“¦ ì‹œë£ŒëŸ‰")
            result.append(f"â€¢ {sample_info}")

        # "-" ë¡œ ì‹œì‘í•˜ëŠ” í•­ëª© ì¶”ì¶œ (ì˜ˆ: - í•­ìƒë¬¼ì§ˆ 28ì¢…)
        dash_items = re.findall(r'-\s*([^-*\n]+?)(?=\s*-|\s*\*|$)', text)
        if dash_items:
            result.append("\nğŸ“‹ ê²€ì‚¬í•­ëª©")
            result.append("")
            for item in dash_items:
                item = item.strip()
                item = re.sub(r'\s+', ' ', item)
                if item and len(item) > 2:
                    result.append(f"â€¢ {item}")

        # * ë¡œ ì‹œì‘í•˜ëŠ” ì°¸ê³  ì‚¬í•­ ì¶”ì¶œ
        notes = re.findall(r'\*\s*([^*]+)', text)
        if notes:
            result.append("\nâš ï¸ ì°¸ê³ ì‚¬í•­")
            result.append("")
            for note in notes:
                note = note.strip()
                note = re.sub(r'\s+', ' ', note)
                if note and len(note) > 3:
                    result.append(f"â€¢ {note}")

        # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬í•´ì„œ ë°˜í™˜
        if not result:
            items = re.split(r'[-â€¢]\s*', text)
            for item in items:
                item = re.sub(r'\s+', ' ', item).strip()
                if item and len(item) > 3:
                    result.append(f"â€¢ {item}")

        return "\n".join(result) if result else text

    def crawl_general_info(self) -> int:
        """
        ì¼ë°˜ ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§ (ìê°€í’ˆì§ˆê²€ì‚¬, ì†Œë¹„ê¸°í•œì„¤ì •, í•­ìƒë¬¼ì§ˆ, ì”ë¥˜ë†ì•½, ë°©ì‚¬ëŠ¥, ë¹„ê±´, í• ë„, ë™ë¬¼DNA, ì•Œë ˆë¥´ê¸°, ê¸€ë£¨í…Free)

        Returns:
            ì €ì¥ëœ í•­ëª© ìˆ˜
        """
        total_count = 0
        categories = [
            "ìê°€í’ˆì§ˆê²€ì‚¬", "ì†Œë¹„ê¸°í•œì„¤ì •",
            "í•­ìƒë¬¼ì§ˆ", "ì”ë¥˜ë†ì•½", "ë°©ì‚¬ëŠ¥",
            "ë¹„ê±´", "í• ë„", "ë™ë¬¼DNA",
            "ì•Œë ˆë¥´ê¸°", "ê¸€ë£¨í…Free"
        ]

        try:
            driver = self._get_driver()
            logger.info("ì¼ë°˜ ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")

            for category in categories:
                category_urls = URL_MAPPING.get(category, {})
                category_popups = GENERAL_POPUP_MAPPING.get(category, {})
                category_filters = SECTION_FILTER.get(category, {})

                for menu_type, url in category_urls.items():
                    target_id = category_popups.get(menu_type)
                    if not target_id:
                        logger.warning(f"{category} íŒì—… IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {menu_type}")
                        continue

                    # ì„¹ì…˜ í•„í„° ê°€ì ¸ì˜¤ê¸°
                    section_filter = category_filters.get(menu_type)

                    logger.info(f"{category} í¬ë¡¤ë§: {menu_type} (URL: {url})" + (f" [í•„í„°: {section_filter}]" if section_filter else ""))

                    driver.get(url)
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )

                    soup = BeautifulSoup(driver.page_source, "html.parser")

                    # íŒì—… ìš”ì†Œì—ì„œ í…Œì´ë¸” ì°¾ê¸°
                    target_element = soup.find("div", class_="needpopup answerPop", id=target_id)
                    if not target_element:
                        logger.warning(f"{category} íŒì—… ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {menu_type} (ID: {target_id})")
                        continue

                    table = target_element.find("table")
                    if not table:
                        # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ ì •ì œ
                        raw_text = target_element.get_text(strip=True)
                        if raw_text:
                            details = self._extract_items_from_text(raw_text, category, section_filter)
                            if details:
                                save_nutrition_info(category, menu_type, details)
                                total_count += 1
                                logger.info(f"{category} ì €ì¥ ì™„ë£Œ: {menu_type} (í…ìŠ¤íŠ¸)")
                        continue

                    # í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (ë§í¬ í¬í•¨)
                    rows = table.find_all("tr")
                    table_data = []

                    for row in rows:
                        cells = row.find_all(["th", "td"])
                        if cells:
                            row_data = self._extract_cell_data_with_links(cells)
                            table_data.append(row_data)

                    if table_data:
                        # ì„¹ì…˜ í•„í„° ì ìš©í•˜ì—¬ í¬ë§·íŒ…
                        details = self._format_table_data(table_data, section_filter)
                        if details:  # í•„í„° ì ìš© í›„ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
                            save_nutrition_info(category, menu_type, details)
                            total_count += 1
                            logger.info(f"{category} ì €ì¥ ì™„ë£Œ: {menu_type}")

            logger.info(f"ì¼ë°˜ ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§ ì™„ë£Œ: {total_count}ê°œ ì €ì¥")
            return total_count

        except Exception as e:
            logger.error(f"ì¼ë°˜ ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return 0

    def crawl_all(self):
        """ëª¨ë“  ë°ì´í„° í¬ë¡¤ë§"""
        logger.info("=" * 50)
        logger.info("ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
        logger.info("=" * 50)

        total = 0

        # ê²€ì‚¬í•­ëª© í¬ë¡¤ë§ (requests - ê°€ë²¼ì›€)
        for category in ["ì‹í’ˆ", "ì¶•ì‚°"]:
            count = self.crawl_inspection_items(category)
            total += count

        # ê²€ì‚¬ì£¼ê¸° í¬ë¡¤ë§ (Selenium - ë™ì  í˜ì´ì§€)
        for category in ["ì‹í’ˆ", "ì¶•ì‚°"]:
            count = self.crawl_inspection_cycles(category)
            total += count

        # ì˜ì–‘ì„±ë¶„ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§
        count = self.crawl_nutrition_info()
        total += count

        # ì¼ë°˜ ê²€ì‚¬ ì •ë³´ í¬ë¡¤ë§ (í•­ìƒë¬¼ì§ˆ, ì”ë¥˜ë†ì•½, ë°©ì‚¬ëŠ¥, ë¹„ê±´, í• ë„, ë™ë¬¼DNA)
        count = self.crawl_general_info()
        total += count

        # WebDriver ì¢…ë£Œ
        self.close()

        # í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥
        if total > 0:
            save_crawl_log("all", "success", f"ì´ {total}ê°œ ë°ì´í„° ì €ì¥")
            logger.info(f"ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {total}ê°œ ë°ì´í„° ì €ì¥")
        else:
            save_crawl_log("all", "failed", "ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
            logger.error("í¬ë¡¤ë§ ì‹¤íŒ¨: ì €ì¥ëœ ë°ì´í„° ì—†ìŒ")

        return total


def run_crawler():
    """í¬ë¡¤ëŸ¬ ì‹¤í–‰ (ì™¸ë¶€ì—ì„œ í˜¸ì¶œìš©)"""
    init_database()
    crawler = Crawler()
    try:
        return crawler.crawl_all()
    finally:
        crawler.close()


if __name__ == "__main__":
    run_crawler()
